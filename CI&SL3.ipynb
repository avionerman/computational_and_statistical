{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rdVI869fZ081",
        "qatQL2mkvl6h",
        "yV8HGfRVwdYc",
        "elDewjFMzHrA",
        "EsnM9XBH0LC7",
        "UlCDLE8RcWTa",
        "Bn3iHvi8yS_t",
        "s9LwMgzL1bbB"
      ],
      "authorship_tag": "ABX9TyPIzPBeRTySNzF+M8H0rLP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avionerman/computational_and_statistical/blob/main/CI%26SL3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libs"
      ],
      "metadata": {
        "id": "rdVI869fZ081"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tRPbjh8ZSqGs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Core scientific computing\n",
        "import numpy as np\n",
        "from numpy.linalg import eigh, eig, svd, norm, inv\n",
        "from scipy.sparse import csr_matrix, diags, eye as sparse_eye\n",
        "from scipy.sparse.linalg import eigsh, eigs\n",
        "from scipy.spatial.distance import cdist, pdist, squareform\n",
        "from scipy.sparse.csgraph import shortest_path\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        "from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import (\n",
        "    adjusted_rand_score,\n",
        "    normalized_mutual_info_score,\n",
        "    homogeneity_score,\n",
        "    completeness_score,\n",
        "    v_measure_score,\n",
        "    confusion_matrix,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# utilities\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from enum import Enum, auto\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# set random seeds for reproducibility for the rest of the code\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration attributes"
      ],
      "metadata": {
        "id": "qatQL2mkvl6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "\n",
        "    # data\n",
        "    sample_fraction: float = 0.10      # using x% of the data for faster experimenting\n",
        "    normalize_method: str = 'minmax'   # avail options: 'minmax', 'standard'\n",
        "    random_seed: int = 42\n",
        "\n",
        "    # graph\n",
        "    n_neighbors: int = 10 # k for kNN graph\n",
        "    sigma: float = None   # gaussian kernel bandwidth (none = auto-estimate)\n",
        "\n",
        "    # enable/disable methods\n",
        "    enable_laplacian_eigenmaps: bool = True\n",
        "    enable_lpp: bool = True\n",
        "    enable_isomap: bool = True\n",
        "    enable_lle: bool = True\n",
        "    enable_tsne: bool = True\n",
        "\n",
        "    # spectral settings\n",
        "    n_clusters_list: List[int] = field(default_factory=lambda: [2, 3, 5, 7, 10])\n",
        "\n",
        "    # tSNE only\n",
        "    tsne_perplexity: float = 30.0\n",
        "    tsne_n_iter: int = 1000\n",
        "\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(\">>> Configuration set\")\n",
        "print(f\"   Sample fraction: {config.sample_fraction * 100:.0f}%\")\n",
        "print(f\"   n_neighbors: {config.n_neighbors}\")\n",
        "print(f\"   Methods enabled: LE={config.enable_laplacian_eigenmaps}, LPP={config.enable_lpp}, \"\n",
        "      f\"Isomap={config.enable_isomap}, LLE={config.enable_lle}, t-SNE={config.enable_tsne}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBhWfVQOvlRE",
        "outputId": "5f42987c-0ef9-48ff-e12f-fac315eb508e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Configuration set\n",
            "   Sample fraction: 10%\n",
            "   n_neighbors: 10\n",
            "   Methods enabled: LE=True, LPP=True, Isomap=True, LLE=True, t-SNE=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "yV8HGfRVwdYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist():\n",
        "    from tensorflow.keras.datasets import mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    class_names = [str(i) for i in range(10)]\n",
        "    return x_train, y_train, x_test, y_test, class_names\n",
        "\n",
        "def load_cifar10():\n",
        "    from tensorflow.keras.datasets import cifar10\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    y_train, y_test = y_train.flatten(), y_test.flatten()\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "    return x_train, y_train, x_test, y_test, class_names\n",
        "\n",
        "def sample_data(x, y, fraction, random_seed=RANDOM_SEED):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    x_sample, _, y_sample, _ = train_test_split(\n",
        "        x, y,\n",
        "        train_size=fraction,\n",
        "        stratify=y, # enable stratification for fair representation\n",
        "        random_state=random_seed\n",
        "    )\n",
        "    return x_sample, y_sample\n",
        "\n",
        "def preprocess_data(x, y, config):\n",
        "    # sample, flatten, normalize to [0,1].\n",
        "\n",
        "    x, y = sample_data(x, y, config.sample_fraction, config.random_seed)\n",
        "\n",
        "    # flatten and normalize (images are 0-255 move under scale to 0-1)\n",
        "    x_flat = x.reshape(len(x), -1).astype(np.float32) / 255.0\n",
        "\n",
        "    return x_flat, y"
      ],
      "metadata": {
        "id": "zBEjTaaLwf7U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "elDewjFMzHrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading MNIST\n",
        "print(\">>> Loading MNIST\")\n",
        "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist, mnist_classes = load_mnist()\n",
        "x_mnist, y_mnist = preprocess_data(x_train_mnist, y_train_mnist, config)\n",
        "print(f\"MNIST: {x_mnist.shape[0]} samples, {x_mnist.shape[1]} features\")\n",
        "\n",
        "# loading CIFAR10\n",
        "print(\">>> Loading CIFAR10\")\n",
        "x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar, cifar_classes = load_cifar10()\n",
        "x_cifar, y_cifar = preprocess_data(x_train_cifar, y_train_cifar, config)\n",
        "print(f\"CIFAR10: {x_cifar.shape[0]} samples, {x_cifar.shape[1]} features\")\n",
        "\n",
        "print(\">>> Data loaded and preprocessed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9zyPjvSzKZH",
        "outputId": "d0ad4bed-bda6-4fd3-95f3-9b8b7ccf23c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Loading MNIST\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "MNIST: 6000 samples, 784 features\n",
            ">>> Loading CIFAR10\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n",
            "CIFAR10: 5000 samples, 3072 features\n",
            ">>> Data loaded and preprocessed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets visualization"
      ],
      "metadata": {
        "id": "EsnM9XBH0LC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_samples(x_original, y, class_names, dataset_name, n_samples=5):\n",
        "    # present sample images from each class\n",
        "    n_classes = len(class_names)\n",
        "    fig, axes = plt.subplots(n_classes, n_samples, figsize=(n_samples * 1.5, n_classes * 1.5))\n",
        "    fig.suptitle(f'{dataset_name}: Sample Images', fontsize=14)\n",
        "\n",
        "    for row, cls in enumerate(range(n_classes)):\n",
        "        # get images of this class\n",
        "        cls_indices = np.where(y == cls)[0]\n",
        "        selected = np.random.choice(cls_indices, size=n_samples, replace=False)\n",
        "\n",
        "        for col, idx in enumerate(selected):\n",
        "            ax = axes[row, col]\n",
        "            img = x_original[idx]\n",
        "\n",
        "            # handling grayscale (MNIST) vs rbg (CIFAR10)\n",
        "            if len(img.shape) == 2:\n",
        "                ax.imshow(img, cmap='gray')\n",
        "            else:\n",
        "                ax.imshow(img)\n",
        "\n",
        "            ax.axis('off')\n",
        "            if col == 0:\n",
        "                ax.set_title(class_names[cls], fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# visualize_samples(x_train_mnist, y_train_mnist, mnist_classes, 'MNIST')\n",
        "print(\"\\n\\n\")\n",
        "# visualize_samples(x_train_cifar, y_train_cifar, cifar_classes, 'CIFAR10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adL7Wwsh0Og9",
        "outputId": "1bb305bb-0afd-4287-8df4-4ab7829215df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph creation"
      ],
      "metadata": {
        "id": "UlCDLE8RcWTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_knn_graph(x, n_neighbors):\n",
        "    \"\"\"\n",
        "    finds k nearest neighbors for each point\n",
        "    returning symmetric binary adjacency matrix\n",
        "    mode -> connectivity: matrix should be binary (1: connection, 0: no connection)\n",
        "    include_self: ensures a point is not considered its own neighbor\n",
        "    \"\"\"\n",
        "    knn = kneighbors_graph(x, n_neighbors, mode='connectivity', include_self=False)\n",
        "\n",
        "    # symmetric transformation:\n",
        "    # if there's an edge from i to j or from j to i\n",
        "    # it will be present in adjacency\n",
        "    adjacency = knn + knn.T\n",
        "\n",
        "    # if both i is neighbor of j and j is neighbor of i, then the entry will be 2\n",
        "    # so I convert them to 1, or I keep them as 0 if it's 0 already\n",
        "    adjacency = (adjacency > 0).astype(np.float64)\n",
        "\n",
        "    return adjacency.toarray()\n",
        "\n",
        "def compute_gaussian_weights(x, adjacency, sigma=None):\n",
        "    \"\"\"\n",
        "    converts distances to similarities using Gaussian kernel\n",
        "    compute Gaussian kernel weights (used formula from pdf)\n",
        "    W_ij = exp(-||x_i - x_j||^2 / (2 * sigma^2))\n",
        "    \"\"\"\n",
        "    distances = cdist(x, x, 'euclidean')\n",
        "\n",
        "    # estimate sigma IF not provided (median)\n",
        "    if sigma is None:\n",
        "\n",
        "      # considers distances between points that are k nearest neighbors\n",
        "        connected_distances = distances[adjacency > 0]\n",
        "\n",
        "        # if sigma is none, will be calculated using distances' median\n",
        "        sigma = np.median(connected_distances)\n",
        "\n",
        "    # closer to 1: better similarity\n",
        "    # closer to 0, the opposite\n",
        "    W = np.exp(-distances**2 / (2 * sigma**2))\n",
        "\n",
        "    # all the entries (where adjacency is 0) become 0\n",
        "    # meaning non-neighbors have zero similarity\n",
        "    W = W * adjacency\n",
        "    np.fill_diagonal(W, 0)\n",
        "\n",
        "    return W, sigma\n",
        "\n",
        "def build_similarity_graph(x, n_neighbors, sigma=None):\n",
        "    \"\"\"\n",
        "    combines both steps + computes degree matrix\n",
        "    and then building complete similarity graph\n",
        "    returning W (matrix), D (matrix) and which sigma used\n",
        "    \"\"\"\n",
        "    adjacency = build_knn_graph(x, n_neighbors)\n",
        "    W, sigma = compute_gaussian_weights(x, adjacency, sigma)\n",
        "    D = np.diag(W.sum(axis=1))  # degree matrix\n",
        "\n",
        "    return W, D, sigma"
      ],
      "metadata": {
        "id": "F8clgqCaccWs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: Laplacian Eigenmap [LE]"
      ],
      "metadata": {
        "id": "Bn3iHvi8yS_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def laplacian_eigenmaps(x, n_components=2, n_neighbors=15, sigma=None):\n",
        "    \"\"\"\n",
        "    Laplacian Eigenmaps for dimensionality reduction:\n",
        "\n",
        "    1. building similarity graph (W and D)\n",
        "    2. calculating Laplacian L = D - W\n",
        "    3. normalization with L_rows = D^(-1) @ L\n",
        "    4. finding the actual eigenvectors\n",
        "    5. returnin at the end the 2nd to (n_components+1)th eigenvectors\n",
        "    \"\"\"\n",
        "    # 1: building similarity graph\n",
        "    W, D, sigma = build_similarity_graph(x, n_neighbors, sigma)\n",
        "\n",
        "    # 2: calculating Laplacian\n",
        "    L = D - W\n",
        "\n",
        "    # 3: normalization with (L_rows = D^(-1) @ L)\n",
        "\n",
        "    # small value to avoid division by zero\n",
        "    D_inv = np.diag(1.0 / (np.diag(D) + 1e-10))\n",
        "    L_rows = D_inv @ L\n",
        "\n",
        "    # 4: finding the actual eigenvectors\n",
        "    eigenvalues, eigenvectors = eigh(L_rows)\n",
        "\n",
        "    # sorting by eigenvalue and skip the first eigenvector (useless)\n",
        "    idx = np.argsort(eigenvalues)\n",
        "    eigenvalues = eigenvalues[idx]\n",
        "    eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "    # 5: returnin at the end the 2nd to (n_components+1)th eigenvectors\n",
        "    embedding = eigenvectors[:, 1:n_components + 1]\n",
        "\n",
        "    return embedding, eigenvalues\n",
        "\n",
        "\n",
        "print(\">>> Laplacian Eigenmaps executed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "innuqwuqyW5d",
        "outputId": "a7cd9743-3d6a-4aa2-d962-7a688a17f5c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Laplacian Eigenmaps executed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: Locality Preserving Projections [LPP]"
      ],
      "metadata": {
        "id": "s9LwMgzL1bbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lpp(x, n_components=2, n_neighbors=15, sigma=None):\n",
        "    \"\"\"\n",
        "    LPP - Linear approximation of Laplacian Eigenmaps\n",
        "\n",
        "    key difference from LE:\n",
        "    - LE gives embedding (positions)\n",
        "    - LPP gives projection matrix (can project new data)\n",
        "\n",
        "    I want to solve the X @ L @ X.T @ w = λ @ X @ D @ X.T @ w\n",
        "    formula was used after reading the following paper:\n",
        "    https://proceedings.neurips.cc/paper/2003/file/d69116f8b0140cdeb1f99a4d5096ffe4-Paper.pdf\n",
        "    \"\"\"\n",
        "    from scipy.linalg import eigh as scipy_eigh\n",
        "\n",
        "    n_samples, n_features = x.shape\n",
        "\n",
        "    # 1: building similarity graph (same as LE but for LPP this time)\n",
        "    W, D, sigma = build_similarity_graph(x, n_neighbors, sigma)\n",
        "\n",
        "    # 2: calculating Laplacian\n",
        "    L = D - W\n",
        "\n",
        "    # 3: calculating scatter matrices\n",
        "    # X.T has shape (features, samples)\n",
        "    # XLXT has shape (features, features)\n",
        "    XLXT = x.T @ L @ x # (784, 784) - what relation exists through Laplacian\n",
        "    XDXT = x.T @ D @ x # w will be (784,) a weight per feature - what relation exists through connectivity\n",
        "\n",
        "    # 4: regularization (avoid singular matrix)\n",
        "    # np.trace(XDXT) = sum of all diagonal elements (matrix size)\n",
        "    reg = 1e-6 * np.trace(XDXT) / n_features # dividing with n_features to get the avg\n",
        "    XDXT_reg = XDXT + reg * np.eye(n_features) # append to the diagonal\n",
        "\n",
        "    # 5: fix generalized eigenvalue problem\n",
        "    # scipy_eigh() is finding the values and vectors of an eigen\n",
        "    # A*v = λ*B*v using this formula\n",
        "    # vectors are the projection weights\n",
        "    # values are the numbers telling me how good each weight is\n",
        "    eigenvalues, eigenvectors = scipy_eigh(XLXT, XDXT_reg)\n",
        "\n",
        "    # 6: find eigenvalues that are NOT near zero (skip meaningless ones)\n",
        "    threshold = 1e-10\n",
        "    valid_idx = np.where(eigenvalues > threshold)[0]\n",
        "\n",
        "    # sort the valid eigenvalues (smallest first)\n",
        "    valid_eigenvalues = eigenvalues[valid_idx]\n",
        "    sorted_valid_idx = valid_idx[np.argsort(valid_eigenvalues)]\n",
        "\n",
        "    # picks the 2 eigenvector columns that have the smallest valid eigenvalues for 2D result\n",
        "    projection_matrix = eigenvectors[:, sorted_valid_idx[:n_components]]\n",
        "\n",
        "    # 7: data projection\n",
        "    \"\"\"\n",
        "            x                  projection_matrix                embedding\n",
        "    | 15000 images    |       |   784 weights   |     =  | 15000 points in 2D |\n",
        "    |                 |   X   |                 |        |                    |\n",
        "    | with 784 pixels |       | for each of 2D  |        | (x, y) coordinates |\n",
        "    \"\"\"\n",
        "    embedding = x @ projection_matrix\n",
        "\n",
        "    return embedding, projection_matrix, eigenvalues\n",
        "\n",
        "\n",
        "def lpp_project_new_data(x_new, projection_matrix):\n",
        "    \"\"\"\n",
        "    projecting new data using learned LPP projection matrix - big diff from LE that needs to recalculate\n",
        "    \"\"\"\n",
        "    return x_new @ projection_matrix\n",
        "\n",
        "\n",
        "print(\">>> LPP executed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQjaX9mF1dz3",
        "outputId": "52fb4f95-19de-47ea-8e32-140231d261b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> LPP executed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 3: Isometric Mapping [ISOMAP]"
      ],
      "metadata": {
        "id": "Rhyxv_2lPoGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def isomap(x, n_components=2, n_neighbors=15):\n",
        "    \"\"\"\n",
        "    Isomap - Isometric Mapping\n",
        "\n",
        "    Key idea:\n",
        "    - using Geodesic distance (along the surface) instead of Euclidean (straight line)\n",
        "    - Good for \"unfolding\" curved data like \"swiss roll\"\n",
        "\n",
        "    How it works:\n",
        "    1. building a k-NN graph (connecting nearby points)\n",
        "    2. calculating the shortest path between all pairs (approximates geodesic distance)\n",
        "    3. applying MDS (multidimensional scaling) to preserve these distances in 2D\n",
        "\n",
        "    the following paper was used:\n",
        "    https://www.science.org/doi/10.1126/science.290.5500.2319\n",
        "    \"\"\"\n",
        "    from sklearn.manifold import Isomap as SklearnIsomap\n",
        "\n",
        "    # sklearn does all them:\n",
        "    # - builds k-NN graph internally\n",
        "    # - computes shortest paths (geodesic distances)\n",
        "    # - applies MDS to get 2D embedding\n",
        "    model = SklearnIsomap(\n",
        "        n_components=n_components,  # output dimensions (2 for 2D)\n",
        "        n_neighbors=n_neighbors     # k in k-NN graph\n",
        "    )\n",
        "\n",
        "    # fit_transform: learn the structure and transform in one step\n",
        "    embedding = model.fit_transform(x)\n",
        "\n",
        "    return embedding, model\n",
        "\n",
        "\n",
        "print(\">>> Isomap executed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQavN_sSQBAo",
        "outputId": "3a65d279-1991-46ee-bb76-52b438ef0be4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Isomap executed\n"
          ]
        }
      ]
    }
  ]
}